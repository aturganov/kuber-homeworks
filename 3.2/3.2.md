# Домашнее задание к занятию "Установка кластера K8s"

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развернутые ВМ с ОС Ubuntu 20.04-lts


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
2. [Документация kubespray](https://kubespray.io/)

-----
```
Установка кластера:
/home/locadm/git/kuber-homeworks/3.2/create_vm.sh 
locadm@netology01:~/git/kuber-homeworks/3.2$ /home/locadm/git/kuber-homeworks/3.2/create_vm.sh
done (34s)
id: epdbm2doah8dip3aehuv
folder_id: b1gkgthf18fqkuii66ht
created_at: "2023-04-02T16:15:30Z"
name: master1
zone_id: ru-central1-b
platform_id: standard-v2
resources:
  memory: "2147483648"
  cores: "2"
  core_fraction: "100"
status: RUNNING
metadata_options:
  gce_http_endpoint: ENABLED
  aws_v1_http_endpoint: ENABLED
  gce_http_token: ENABLED
  aws_v1_http_token: DISABLED
boot_disk:
  mode: READ_WRITE
  device_name: epdtbrj3mbrpautkrude
  auto_delete: true
  disk_id: epdtbrj3mbrpautkrude
network_interfaces:
  - index: "0"
    mac_address: d0:0d:bb:09:b8:54
    subnet_id: e2l6ospinqqt1ciopklc
    primary_v4_address:
      address: 10.1.2.22
      one_to_one_nat:
        address: 51.250.31.52
        ip_version: IPV4
gpu_settings: {}
fqdn: master1.ru-central1.internal
scheduling_policy: {}
network_settings:
  type: STANDARD
placement_policy: {}

done (31s)
id: epdunohb82b0d9c40sg4
folder_id: b1gkgthf18fqkuii66ht
created_at: "2023-04-02T16:16:05Z"
name: worker1
zone_id: ru-central1-b
platform_id: standard-v2
resources:
  memory: "2147483648"
  cores: "2"
  core_fraction: "100"
status: RUNNING
metadata_options:
  gce_http_endpoint: ENABLED
  aws_v1_http_endpoint: ENABLED
  gce_http_token: ENABLED
  aws_v1_http_token: DISABLED
boot_disk:
  mode: READ_WRITE
  device_name: epdpfsqhnaeotmics5po
  auto_delete: true
  disk_id: epdpfsqhnaeotmics5po
network_interfaces:
  - index: "0"
    mac_address: d0:0d:1e:be:22:b4
    subnet_id: e2l6ospinqqt1ciopklc
    primary_v4_address:
      address: 10.1.2.14
      one_to_one_nat:
        address: 158.160.12.12
        ip_version: IPV4
gpu_settings: {}
fqdn: worker1.ru-central1.internal
scheduling_policy: {}
network_settings:
  type: STANDARD
placement_policy: {}

done (39s)
id: epdoe15nvdhm6909mukv
folder_id: b1gkgthf18fqkuii66ht
created_at: "2023-04-02T16:16:38Z"
name: worker2
zone_id: ru-central1-b
platform_id: standard-v2
resources:
  memory: "2147483648"
  cores: "2"
  core_fraction: "100"
status: RUNNING
metadata_options:
  gce_http_endpoint: ENABLED
  aws_v1_http_endpoint: ENABLED
  gce_http_token: ENABLED
  aws_v1_http_token: DISABLED
boot_disk:
  mode: READ_WRITE
  device_name: epd8nmfuehkldjfuc748
  auto_delete: true
  disk_id: epd8nmfuehkldjfuc748
network_interfaces:
  - index: "0"
    mac_address: d0:0d:18:70:4b:7f
    subnet_id: e2l6ospinqqt1ciopklc
    primary_v4_address:
      address: 10.1.2.10
      one_to_one_nat:
        address: 158.160.12.213
        ip_version: IPV4
gpu_settings: {}
fqdn: worker2.ru-central1.internal
scheduling_policy: {}
network_settings:
  type: STANDARD
placement_policy: {}

locadm@netology01:~/git/kuber-homeworks/3.2$ yc compute instance list
+----------------------+---------+---------------+---------+----------------+-------------+
|          ID          |  NAME   |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
+----------------------+---------+---------------+---------+----------------+-------------+
| epdbm2doah8dip3aehuv | master1 | ru-central1-b | RUNNING | 51.250.31.52   | 10.1.2.22   |
| epdoe15nvdhm6909mukv | worker2 | ru-central1-b | RUNNING | 158.160.12.213 | 10.1.2.10   |
| epdunohb82b0d9c40sg4 | worker1 | ru-central1-b | RUNNING | 158.160.12.12  | 10.1.2.14   |
+----------------------+---------+---------------+---------+----------------+-------------+
```
Вход на админскую машину (master1)
```
    ssh yc-user@51.250.31.52
    sudo apt-get update
    sudo apt-get install -y apt-transport-https ca-certificates curl
    sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
    echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
    
    sudo apt-get update
    sudo apt-get install -y kubelet kubeadm kubectl containerd
    # Холдим от апдейта системы
    sudo apt-mark hold kubelet kubeadm kubectl
```
Инициализация кластера
```
 yc-user@master1:~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether d0:0d:bb:09:b8:54 brd ff:ff:ff:ff:ff:ff
    inet 10.1.2.22/24 brd 10.1.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::d20d:bbff:fe09:b854/64 scope link 
       valid_lft forever preferred_lft forever

kubeadm init \
  --apiserver-advertise-address=10.1.2.22 \
  --pod-network-cidr 10.244.0.0/16 \
  --apiserver-cert-extra-sans=51.250.31.52
```
contrpane-point убераем - будет один мастер
```
ip-forward не включен
Включаем форвард
sudo -i
modprobe br_netfilter 
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-iptables=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-arptables=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-ip6tables=1" >> /etc/sysctl.conf

#Дополнительная команда
echo 1 > /proc/sys/net/ipv4/ip_forward
```

Ссылка на join worker-ов:
```
sudo kubeadm join 10.1.2.22:6443 --token bgdzcb.uho4aft9ywer7v34 \
        --discovery-token-ca-cert-hash sha256:29640e8cd7cc8db9a4fbc50ee03805a13cc009e7ba030339368c21de75739f21 
```
Копирование локальной папки куб для non-root user
```
На мастере
{
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
}
```
Проверка нодов
```
yc-user@master1:~$ kubectl get nodes
NAME      STATUS     ROLES           AGE    VERSION
master1   NotReady   control-plane   10m    v1.26.3
worker1   NotReady   <none>          8m8s   v1.26.3
worker2   NotReady   <none>          16s    v1.26.3
```
Причина не работы нодов
```
yc-user@master1:~$ kubectl describe nodes master1 | grep KubeletNotReady
  Ready            False   Sun, 02 Apr 2023 19:17:32 +0000   Sun, 02 Apr 2023 19:07:08 +0000   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
```
Установка плагина сети
```
yc-user@master1:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```
Подключение через локального пользователя
```
locadm@netology01:~/.kube$ kubectl get nodes
NAME      STATUS   ROLES           AGE   VERSION
master1   Ready    control-plane   29m   v1.26.3
worker1   Ready    <none>          27m   v1.26.3
worker2   Ready    <none>          19m   v1.26.3
```
### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.
```
Скрип создания нодов sh. А описание процедуры выше.
```
[create_vm.sh](create_vm.sh)

yc-user@master1:~$ kubectl get nodes
NAME      STATUS   ROLES           AGE    VERSION
master1   Ready    control-plane   16m    v1.26.3
worker1   Ready    <none>          13m    v1.26.3
worker2   Ready    <none>          6m2s   v1.26.3
```
2. В качестве CRI — containerd.
```
Выше.
```
3. Запуск etcd производить на мастере.

```
Под etcd на мастере в списке ниже.
yc-user@master1:~$ kubectl get pods -A -o wide
NAMESPACE      NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-4mbcm             1/1     Running   0          23m   10.1.2.10    worker2   <none>           <none>
kube-flannel   kube-flannel-ds-cdp26             1/1     Running   0          23m   10.1.2.22    master1   <none>           <none>
kube-flannel   kube-flannel-ds-ph5nw             1/1     Running   0          23m   10.1.2.14    worker1   <none>           <none>
kube-system    coredns-787d4945fb-2mk85          1/1     Running   0          38m   10.244.1.2   worker1   <none>           <none>
kube-system    coredns-787d4945fb-nw9mp          1/1     Running   0          38m   10.244.0.2   master1   <none>           <none>
kube-system    etcd-master1                      1/1     Running   0          38m   10.1.2.22    master1   <none>           <none>
kube-system    kube-apiserver-master1            1/1     Running   0          38m   10.1.2.22    master1   <none>           <none>
kube-system    kube-controller-manager-master1   1/1     Running   0          38m   10.1.2.22    master1   <none>           <none>
kube-system    kube-proxy-j8d8q                  1/1     Running   0          28m   10.1.2.10    worker2   <none>           <none>
kube-system    kube-proxy-lwhgg                  1/1     Running   0          38m   10.1.2.22    master1   <none>           <none>
kube-system    kube-proxy-p97d9                  1/1     Running   0          35m   10.1.2.14    worker1   <none>           <none>
kube-system    kube-scheduler-master1            1/1     Running   0          38m   10.1.2.22    master1   <none>           <none>
```

4. Способ установки выбрать самостоятельно.

```
Вариант описан выше.
```

## Дополнительные задания (со звездочкой*)

**Настоятельно рекомендуем выполнять все задания под звёздочкой.**   Их выполнение поможет глубже разобраться в материале.   
Задания под звёздочкой дополнительные (необязательные к выполнению) и никак не повлияют на получение вами зачета по этому домашнему заданию. 

------
### Задание 2*. Установить HA кластер

1. Установить кластер в режиме HA
2. Использовать нечетное кол-во Master-node
3. Для cluster ip использовать keepalived или другой способ

### Правила приема работы

1. Домашняя работа оформляется в своем Git репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд `kubectl get nodes`, а также скриншоты результатов
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md