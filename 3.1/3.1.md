# Домашнее задание к занятию "Архитектура кластера k8s"

### Цель задания

Рассчитать требования к кластеру под проект   

------

### Инструменты/ дополнительные материалы, которые пригодятся для выполнения задания

- [Considerations for large clusters](https://kubernetes.io/docs/setup/best-practices/cluster-large/)
- [Architecting Kubernetes clusters — choosing a worker node size](https://learnk8s.io/kubernetes-node-size)

------

### Задание. Необходимо определить требуемые ресурсы. 
Известно, что проекту нужны база данных, система кеширования, а само приложение состоит из бекенда и фронтенда. Опишите, какие ресурсы нужны, если известно:

1. Необходимо упаковать приложение в чарт для деплоя в разные окружения. 
2. База данных должна быть отказоустойчивой. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии. 
3. Кэш должен быть отказоустойчивый. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии. 
4. Фронтенд обрабатывает внешние запросы быстро, отдавая статику. Потребляет не более 50 МБ ОЗУ на каждый экземпляр, 0.2 ядра. 5 копий. 
5. Бекенд потребляет 600 МБ ОЗУ и по 1 ядру на копию. 10 копий.

```
Краеугольный камень. Чем больше нод, рабочих машин, тем больше технических устройство и их поддержки, хотя ноды могут быть и на одно машине, если говорить о виртуалках.
Далее чем больше нод, тем больше ОС нужно поддерживать (обновления и т.д.). Выше стоимость решения. Если мало нод, то меньше ошибок. Но влияние одной ноды на другие повышается.
Количество нодов в свете вопроса отказоустойчивости должно быть нечетным. 
Чем ближе количество реплик к количесту нод, тем также выше отказоустойчивость. В идеале одна реплика - один нод.
Чем меньше нод, тем дискретность наращивания ресурсов выше. Чем больше нод, при прочих равных, тем больше ресурсов требуется на интеграцию стурктуры, в частности затраты DS выше.
```
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/
https://docs.kublr.com/installation/hardware-recommendation/
```

Если предположить, что ресурсы указаны во всех позициях на экземляр (а явно не указано в пункте 1 и 2). 
Рассчитаем суммарное количество требуемых ресурсов.

1. База - 3 реплики. Всего: CPU-3, RAM-12
2. Кэш - 3 реплики. Всего: CPU-3, RAM-12
3. Фронтэнд - 5 реплик. Всего: CPU-1, RAM-0,25
4. Бекэнд - 10 реплик. Всего: CPU-10, RAM-6
Итого: CPU-7, RAM-30,25 

Логика расчета с учетом затрат на сервисные функции: 

Available memory = (number of nodes) × (memory per node) - (number of nodes) × 0.7GB - (has Self-hosted logging) × 9GB - (has Self-hosted monitoring) × 2.9GB - 0.4 GB - 2GB (Central monitoring agent per every cluster).

Available CPU = (number of nodes) × (vCPU per node) - (number of nodes) × 0.5 - (has Self-hosted logging) × 1 - (has Self-hosted monitoring) × 1.4 - 0.1 - 0.7 (Central monitoring agent per every cluster).

Если ориентироваться на дискретность Амазон/Гугл (просто дискретность сервиса, можно его поменять) 4 cpu, 16GB на ворк.  Включая блок мониторинга и логирования. И мастер нода может быть 2cpu, 4/8 ГБ.

Сайзинг:

При 3-х нодах (нечетно), расчет доступной нагрузки:
Available memory = 3 × 16 - 3 × 0.7 - 1 × 9 - 1 × 2.8 - 0.4 - 2 = 31 GB
Available CPU = 5 × 4 - 5 × 0.5 - 1 × 1 - 1 × 1.4 - 0.1 - 0.7 = 9 vCPUs.

При 5-х нодах (нечетно), расчет нагрузки:
Available memory = 5 × 16 - 5 × 0.7 - 1 ×9 - 1 × 2.8 - 0.4 - 2 = 62 GB
Available CPU = 5 × 4 - 5 × 0.5 - 1 × 1 - 1 × 1.4 - 0.1 - 0.7 = 16 vCPUs.

Если обеспечивать максимальный уровень непрерывности и отказоустойчивости работы, лучше раскидать по количеству копий на фронт энд и рассмотреть стандартный кластер из 5 нод (нечетной). А также есть возможность реализовать стратегии переноса функционала, типа ab и т.д. И также учитывать разбивку по пространствам микросервисов.
Если в качестве типовой на AWS то готовый кластер типа 
 n1-standard-4 и мастер n1-standard-2 (гугл классификация). 

3-х нодный исходя из дискретности ресурсов (если брать AWS) в притык, и не будет позволять охватывать рост нагрузки, производит перенос версий, нет запаса хотя бы на одну ноду. Низкая отказоустойчивость, много подов различных на нод. 

По жизни можно стартовать с 3-х нод, а дальше масшабировать по итогам детального анализа загрузки и работы сервисов. И перейти с 3-х на 5-х. На то он куберкластер и нужен, чтобы масшабироваться )
```

----

### Правила приема работы

1. Домашняя работа оформляется в своем Git репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Сначала сделайте расчет всех необходимых ресурсов 
3. Затем прикиньте количество рабочих нод, которые справятся с такой нагрузкой. 
4. Добавьте к полученным цифрам запас, который учитывает выход из строя как минимум одной ноды. 
5. Добавьте служебные ресурсы к нодам. Помните, что для разных типов нод требовния к ресурсам разные. 
6. В результате должно быть указано количество нод и их параметры.
