

Установка кластера c Calico через Cubespray

[create_vm.sh](create_vm.sh)
```
git clone https://github.com/kubernetes-sigs/kubespray

yc-user@ks-master1:~/kubespray$ sudo apt install pip
yc-user@ks-master1:~/kubespray$ sudo pip3 install -r requirements.txt

+----------------------+------------+---------------+---------+----------------+-------------+
|          ID          |    NAME    |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
+----------------------+------------+---------------+---------+----------------+-------------+
| epdijln1il4p1fjkjgj1 | ks-worker3 | ru-central1-b | RUNNING | 51.250.18.146  | 10.1.2.3    |
| epdlc0qeguvvbjsl5k9j | ks-worker1 | ru-central1-b | RUNNING | 51.250.96.86   | 10.1.2.6    |
| epdpocaet9adlcl0e3qf | ks-worker2 | ru-central1-b | RUNNING | 62.84.121.197  | 10.1.2.32   |
| epdqgif8mp41vthemmig | ks-master1 | ru-central1-b | RUNNING | 51.250.24.52   | 10.1.2.10   |
+----------------------+------------+---------------+---------+----------------+-------------+

declare -a IPS=(51.250.24.52 51.250.96.86 51.250.18.146 62.84.121.197)

declare -a IPS=(10.1.2.10 10.1.2.6 10.1.2.32 10.1.2.3)

yc-user@ks-master1:~/kubespray$ CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
DEBUG: Adding group all
DEBUG: Adding group kube_control_plane
DEBUG: Adding group kube_node
DEBUG: Adding group etcd
DEBUG: Adding group k8s_cluster
DEBUG: Adding group calico_rr
DEBUG: adding host node1 to group all
DEBUG: adding host node2 to group all
DEBUG: adding host node3 to group all
DEBUG: adding host node4 to group all
DEBUG: adding host node1 to group etcd
DEBUG: adding host node2 to group etcd
DEBUG: adding host node3 to group etcd
DEBUG: adding host node1 to group kube_control_plane
DEBUG: adding host node2 to group kube_control_plane
DEBUG: adding host node1 to group kube_node
DEBUG: adding host node2 to group kube_node
DEBUG: adding host node3 to group kube_node
DEBUG: adding host node4 to group kube_node

yc-user@ks-master1:~/kubespray$ vim ~/.ssh/id_rsa
yc-user@ks-master1:~/kubespray$ chmod 0600 ~/.ssh/id_rsa
```
Настройки куба  
[inventory/mycluster/hosts.yaml](inventory/mycluster/hosts.yaml)
```

yc-user@master1:~/3.3/main$ kubectl get pods -A
NAMESPACE     NAME                                      READY   STATUS    RESTARTS        AGE
app           backend-7f6ffd4fb4-7nnd9                  1/1     Running   0               8m29s
app           cache-7bb8f4764b-ll5sc                    1/1     Running   0               8m17s
app           frontend-85f54fff68-j6qcz                 1/1     Running   0               8m38s
kube-system   calico-kube-controllers-6dfcdfb99-gntrw   1/1     Running   0               3d18h
kube-system   calico-node-89s2p                         1/1     Running   0               3d18h
kube-system   calico-node-99b5m                         1/1     Running   0               3d18h
kube-system   calico-node-9lptb                         1/1     Running   0               3d18h
kube-system   calico-node-sgk94                         1/1     Running   0               3d18h
kube-system   coredns-645b46f4b6-8m2l4                  1/1     Running   0               3d18h
kube-system   coredns-645b46f4b6-wkj9f                  1/1     Running   0               3d18h
kube-system   dns-autoscaler-65d5b47988-tg5w4           1/1     Running   0               3d18h
kube-system   kube-apiserver-master1                    1/1     Running   1               3d18h
kube-system   kube-controller-manager-master1           1/1     Running   2               3d18h
kube-system   kube-proxy-7nzbt                          1/1     Running   0               3d18h
kube-system   kube-proxy-d4qlx                          1/1     Running   0               3d18h
kube-system   kube-proxy-mb54c                          1/1     Running   0               3d18h
kube-system   kube-proxy-wnlff                          1/1     Running   0               3d18h
kube-system   kube-scheduler-master1                    1/1     Running   2 (3d18h ago)   3d18h
kube-system   nginx-proxy-node2                         1/1     Running   0               3d18h
kube-system   nginx-proxy-node3                         1/1     Running   0               3d18h
kube-system   nginx-proxy-node4                         1/1     Running   0               3d18h
kube-system   nodelocaldns-62hd7                        1/1     Running   0               3d18h
kube-system   nodelocaldns-rvj4l                        1/1     Running   0               3d18h
kube-system   nodelocaldns-wlhx2                        1/1     Running   0               3d18h
kube-system   nodelocaldns-zrgl4                        1/1     Running   0               3d18h


yc-user@master1:~/3.3/main$ kubectl get deployments -o wide
NAME       READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS          IMAGES                                  SELECTOR
backend    1/1     1            1           111m   network-multitool   praqma/network-multitool:alpine-extra   app=backend
cache      1/1     1            1           111m   network-multitool   praqma/network-multitool:alpine-extra   app=cache
frontend   1/1     1            1           111m   network-multitool   praqma/network-multitool:alpine-extra   app=frontend

Сервисы:
yc-user@master1:~/3.3/network-policy$ kubectl get svc
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
backend-svc    ClusterIP   10.233.50.57    <none>        80/TCP    140m
cache-svc      ClusterIP   10.233.16.187   <none>        80/TCP    140m
frontend-svc   ClusterIP   10.233.50.179   <none>        80/TCP    141m
kubernetes     ClusterIP   10.233.0.1      <none>        443/TCP   3d20h
```
Манифесты  
[10-frontend.yaml](main/10-frontend.yaml)  
[20-backend.yaml](main/20-backend.yaml)  
[30-cache.yaml](main/30-cache.yaml)


```
Работа dns: 
yc-user@master1:~/3.3/main$ kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- nslookup kubernetes.default
Server:         169.254.25.10
Address:        169.254.25.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.233.0.1

yc-user@master1:~/3.3/main$ kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- nslookup frontend-svc
Server:         169.254.25.10
Address:        169.254.25.10#53

Name:   frontend-svc.default.svc.cluster.local
Address: 10.233.50.179

Все связи открыты:
yc-user@master1:~/3.3/main$ kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- curl frontend-svc
Praqma Network MultiTool (with NGINX) - frontend-85f54fff68-j6qcz - 10.233.74.65
yc-user@master1:~/3.3/main$  kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- curl cache-svc
Praqma Network MultiTool (with NGINX) - cache-7bb8f4764b-ll5sc - 10.233.71.2
yc-user@master1:~/3.3/main$ kubectl exec -ti cache-7bb8f4764b-ll5sc -- curl frontend-svc
Praqma Network MultiTool (with NGINX) - frontend-85f54fff68-j6qcz - 10.233.74.65
yc-user@master1:~/3.3/main$ kubectl exec -ti cache-7bb8f4764b-ll5sc -- curl backend-svc
Praqma Network MultiTool (with NGINX) - backend-7f6ffd4fb4-7nnd9 - 10.233.75.2
yc-user@master1:~/3.3/main$ kubectl exec -ti frontend-85f54fff68-j6qcz -- curl backend-svc
Praqma Network MultiTool (with NGINX) - backend-7f6ffd4fb4-7nnd9 - 10.233.75.2
yc-user@master1:~/3.3/main$ kubectl exec -ti frontend-85f54fff68-j6qcz -- curl cache-svc
Praqma Network MultiTool (with NGINX) - cache-7bb8f4764b-ll5sc - 10.233.71.2
```
```
Запрет всех связей 
yc-user@master1:~/3.3/network-policy$ kubectl apply -f 00-default.yaml 
networkpolicy.networking.k8s.io/default-deny-ingress created\

Пример:
yc-user@master1:~/3.3/network-policy$ kubectl exec -ti frontend-85f54fff68-j6qcz -- curl cache-svc
curl: (28) Failed to connect to cache-svc port 80 after 129272 ms: Operation timed out
command terminated with exit code 28


Установка Манифестов Network Policy
yc-user@master1:~/3.3/network-policy$ kubectl apply -f front_back.yaml 
networkpolicy.networking.k8s.io/frontend created
yc-user@master1:~/3.3/network-policy$ kubectl apply -f back_cache.yaml 
networkpolicy.networking.k8s.io/backend created

yc-user@master1:~/3.3/network-policy$ kubectl get networkpolicy
NAME                   POD-SELECTOR   AGE
backend                app=backend    21s
cache                  app=cache      2m30s
default-deny-ingress   <none>         24m
```
Манифесты Network-policy  
[default.yaml](nework-policy/default.yaml)  
[front-back.yaml](nework-policy/front-back.yaml)  
[back-cache.yaml](nework-policy/back-cache.yaml)  

```
Тестирование трафика после введение Network Policy
yc-user@master1:~/3.3/network-policy$ kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- curl cache-svc
Praqma Network MultiTool (with NGINX) - cache-7bb8f4764b-ll5sc - 10.233.71.2
yc-user@master1:~/3.3/network-policy$ kubectl exec -ti frontend-85f54fff68-j6qcz -- curl backend-svc
Praqma Network MultiTool (with NGINX) - backend-7f6ffd4fb4-7nnd9 - 10.233.75.2

Другие направления не работают, пример:
yc-user@master1:~/3.3/network-policy$ kubectl exec -ti backend-7f6ffd4fb4-7nnd9 -- curl frontend-svc
curl: (28) Failed to connect to frontend-svc port 80 after 129940 ms: Operation timed out
command terminated with exit code 28
```

# Домашнее задание к занятию "Как работает сеть в K8S"

### Цель задания

Настроить сетевую политику доступа к подам.

### Чеклист готовности к домашнему заданию

1. Кластер k8s с установленным сетевым плагином calico

### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Документация Calico](https://www.tigera.io/project-calico/)
2. [Network Policy](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
3. [About Network Policy](https://docs.projectcalico.org/about/about-network-policy)


### Задание 1. Создать сетевую политику (или несколько политик) для обеспечения доступа

1. Создать deployment'ы приложений frontend, backend и cache и соответсвующие сервисы.
2. В качестве образа использовать network-multitool.
3. Разместить поды в namespace app.
4. Создать политики чтобы обеспечить доступ frontend -> backend -> cache. Другие виды подключений должны быть запрещены.
5. Продемонстрировать, что трафик разрешен и запрещен.
```
см. выше
```

### Правила приема работы

1. Домашняя работа оформляется в своем Git репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд, а также скриншоты результатов
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md